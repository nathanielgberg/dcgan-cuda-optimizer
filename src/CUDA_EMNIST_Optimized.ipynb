{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "av4sMS0uOIER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b629253-88bf-4392-e226-3cf45533511f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should return True\n",
        "print(torch.cuda.get_device_name(0))  # Check GPU type"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda_kernels_optimized.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <math_constants.h>\n",
        "\n",
        "__global__ void double_elements_kernel(float* input, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        input[idx] = input[idx] * 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void double_elements_backward_kernel(float* grad_output, float* grad_input, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        grad_input[idx] = grad_output[idx] * 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_double_elements(float* input, int n) {\n",
        "        int blockSize = 256;\n",
        "        int gridSize = (n + blockSize - 1) / blockSize;\n",
        "        double_elements_kernel<<<gridSize, blockSize>>>(input, n);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    void launch_double_elements_backward(float* grad_output, float* grad_input, int n) {\n",
        "        int blockSize = 256;\n",
        "        int gridSize = (n + blockSize - 1) / blockSize;\n",
        "        double_elements_backward_kernel<<<gridSize, blockSize>>>(grad_output, grad_input, n);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void conv_transpose_general_kernel(const float* input, const float* weight, float* output,\n",
        "    int batch, int in_channels, int out_channels,\n",
        "    int H_in, int W_in,\n",
        "    int H_out, int W_out,\n",
        "    int kH, int kW,\n",
        "    int stride, int padding) {\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total = batch * out_channels * H_out * W_out;\n",
        "    if (idx < total) {\n",
        "        // Decode the output index into (n, oc, y, x)\n",
        "        int temp = idx;\n",
        "        int x = temp % W_out;\n",
        "        temp /= W_out;\n",
        "        int y = temp % H_out;\n",
        "        temp /= H_out;\n",
        "        int oc = temp % out_channels;\n",
        "        int n = temp / out_channels;\n",
        "\n",
        "        float sum = 0.0f;\n",
        "        for (int ic = 0; ic < in_channels; ic++) {\n",
        "            for (int ky = 0; ky < kH; ky++) {\n",
        "                for (int kx = 0; kx < kW; kx++) {\n",
        "                    int i_y = y + padding - ky;\n",
        "                    int i_x = x + padding - kx;\n",
        "                    if (i_y % stride == 0 && i_x % stride == 0) {\n",
        "                        i_y /= stride;\n",
        "                        i_x /= stride;\n",
        "                        if (i_y >= 0 && i_y < H_in && i_x >= 0 && i_x < W_in) {\n",
        "                            int input_idx = ((n * in_channels + ic) * H_in + i_y) * W_in + i_x;\n",
        "                            int flipped_ky = kH - 1 - ky;\n",
        "                            int flipped_kx = kW - 1 - kx;\n",
        "                            int weight_idx = ((ic * out_channels + oc) * kH + flipped_ky) * kW + flipped_kx;\n",
        "                            sum += input[input_idx] * weight[weight_idx];\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        output[idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_conv_transpose_general(const float* input, const float* weight, float* output,\n",
        "        int batch, int in_channels, int out_channels,\n",
        "        int H_in, int W_in,\n",
        "        int H_out, int W_out,\n",
        "        int kH, int kW,\n",
        "        int stride, int padding) {\n",
        "\n",
        "        int total = batch * out_channels * H_out * W_out;\n",
        "        int blockSize = 256;\n",
        "        int gridSize = (total + blockSize - 1) / blockSize;\n",
        "        conv_transpose_general_kernel<<<gridSize, blockSize>>>(input, weight, output,\n",
        "            batch, in_channels, out_channels,\n",
        "            H_in, W_in,\n",
        "            H_out, W_out,\n",
        "            kH, kW,\n",
        "            stride, padding);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void batchnorm_compute_mean_var(const float* input, float* mean, float* var, int N, int C, int H, int W) {\n",
        "    int c = blockIdx.x; // one block per channel\n",
        "    int channel_size = N * H * W;\n",
        "    __shared__ float shared_sum[256];\n",
        "    float sum = 0.0f;\n",
        "    for (int i = threadIdx.x; i < channel_size; i += blockDim.x) {\n",
        "        int n = i / (H * W);\n",
        "        int rem = i % (H * W);\n",
        "        int h = rem / W;\n",
        "        int w = rem % W;\n",
        "        int idx = ((n * C + c) * H + h) * W + w;\n",
        "        sum += input[idx];\n",
        "    }\n",
        "    shared_sum[threadIdx.x] = sum;\n",
        "    __syncthreads();\n",
        "    // Reduce within block.\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n",
        "        if (threadIdx.x < stride)\n",
        "            shared_sum[threadIdx.x] += shared_sum[threadIdx.x + stride];\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (threadIdx.x == 0)\n",
        "        mean[c] = shared_sum[0] / channel_size;\n",
        "    __syncthreads();\n",
        "\n",
        "    __shared__ float shared_sum2[256];\n",
        "    float sum_sq = 0.0f;\n",
        "    float m = mean[c];\n",
        "    for (int i = threadIdx.x; i < channel_size; i += blockDim.x) {\n",
        "        int n = i / (H * W);\n",
        "        int rem = i % (H * W);\n",
        "        int h = rem / W;\n",
        "        int w = rem % W;\n",
        "        int idx = ((n * C + c) * H + h) * W + w;\n",
        "        float diff = input[idx] - m;\n",
        "        sum_sq += diff * diff;\n",
        "    }\n",
        "    shared_sum2[threadIdx.x] = sum_sq;\n",
        "    __syncthreads();\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n",
        "        if (threadIdx.x < stride)\n",
        "            shared_sum2[threadIdx.x] += shared_sum2[threadIdx.x + stride];\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (threadIdx.x == 0)\n",
        "        var[c] = shared_sum2[0] / channel_size;\n",
        "}\n",
        "\n",
        "__global__ void batchnorm_apply(const float* input, float* output, const float* mean, const float* var,\n",
        "                                const float* gamma, const float* beta, int N, int C, int H, int W, float eps) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total = N * C * H * W;\n",
        "    if (idx < total) {\n",
        "        int n = idx / (C * H * W);\n",
        "        int rem = idx % (C * H * W);\n",
        "        int c = rem / (H * W);\n",
        "        int r = rem % (H * W);\n",
        "        int h = r / W;\n",
        "        int w = r % W;\n",
        "        int index = ((n * C + c) * H + h) * W + w;\n",
        "        float m = mean[c];\n",
        "        float v = var[c];\n",
        "        float norm = (input[index] - m) / sqrtf(v + eps);\n",
        "        output[index] = gamma[c] * norm + beta[c];\n",
        "    }\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_batchnorm(const float* input, float* output, const float* gamma, const float* beta,\n",
        "                          int N, int C, int H, int W, float eps) {\n",
        "        int channel_size = N * H * W;\n",
        "        float *mean, *var;\n",
        "        size_t size = C * sizeof(float);\n",
        "        cudaMalloc(&mean, size);\n",
        "        cudaMalloc(&var, size);\n",
        "\n",
        "        int blockSize = 256;\n",
        "        int gridSize = C;  // one block per channel.\n",
        "        batchnorm_compute_mean_var<<<gridSize, blockSize>>>(input, mean, var, N, C, H, W);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        int total = N * C * H * W;\n",
        "        blockSize = 256;\n",
        "        gridSize = (total + blockSize - 1) / blockSize;\n",
        "        batchnorm_apply<<<gridSize, blockSize>>>(input, output, mean, var, gamma, beta, N, C, H, W, eps);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        cudaFree(mean);\n",
        "        cudaFree(var);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void conv2d_kernel(const float* input, const float* weight, float* output,\n",
        "    int batch, int in_channels, int out_channels,\n",
        "    int H_in, int W_in,\n",
        "    int H_out, int W_out,\n",
        "    int kH, int kW,\n",
        "    int stride, int padding) {\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total = batch * out_channels * H_out * W_out;\n",
        "    if (idx < total) {\n",
        "        // Decode index into (n, oc, y, x)\n",
        "        int temp = idx;\n",
        "        int x = temp % W_out; temp /= W_out;\n",
        "        int y = temp % H_out; temp /= H_out;\n",
        "        int oc = temp % out_channels; temp /= out_channels;\n",
        "        int n = temp;\n",
        "\n",
        "        float sum = 0.0f;\n",
        "        // For each input channel and kernel element:\n",
        "        for (int ic = 0; ic < in_channels; ic++) {\n",
        "            for (int ky = 0; ky < kH; ky++) {\n",
        "                for (int kx = 0; kx < kW; kx++) {\n",
        "                    int in_y = y * stride - padding + ky;\n",
        "                    int in_x = x * stride - padding + kx;\n",
        "                    if (in_y >= 0 && in_y < H_in && in_x >= 0 && in_x < W_in) {\n",
        "                        int input_idx = ((n * in_channels + ic) * H_in + in_y) * W_in + in_x;\n",
        "                        // Weight shape: (out_channels, in_channels, kH, kW)\n",
        "                        int weight_idx = ((oc * in_channels + ic) * kH + ky) * kW + kx;\n",
        "                        sum += input[input_idx] * weight[weight_idx];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        output[idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_conv2d(const float* input, const float* weight, float* output,\n",
        "         int batch, int in_channels, int out_channels,\n",
        "         int H_in, int W_in,\n",
        "         int H_out, int W_out,\n",
        "         int kH, int kW,\n",
        "         int stride, int padding) {\n",
        "         int total = batch * out_channels * H_out * W_out;\n",
        "         int blockSize = 256;\n",
        "         int gridSize = (total + blockSize - 1) / blockSize;\n",
        "         conv2d_kernel<<<gridSize, blockSize>>>(input, weight, output,\n",
        "             batch, in_channels, out_channels,\n",
        "             H_in, W_in,\n",
        "             H_out, W_out,\n",
        "             kH, kW,\n",
        "             stride, padding);\n",
        "         cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void relu_kernel(const float* input, float* output, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        float x = input[idx];\n",
        "        output[idx] = (x > 0.0f) ? x : 0.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void relu_backward_kernel(const float* input, const float* grad_output, float* grad_input, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        grad_input[idx] = (input[idx] > 0.0f) ? grad_output[idx] : 0.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_relu(const float* input, float* output, int n) {\n",
        "        int blockSize = 256;\n",
        "        int gridSize = (n + blockSize - 1) / blockSize;\n",
        "        relu_kernel<<<gridSize, blockSize>>>(input, output, n);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    void launch_relu_backward(const float* input, const float* grad_output, float* grad_input, int n) {\n",
        "        int blockSize = 256;\n",
        "        int gridSize = (n + blockSize - 1) / blockSize;\n",
        "        relu_backward_kernel<<<gridSize, blockSize>>>(input, grad_output, grad_input, n);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void conv_transpose2d_backward_input_kernel(\n",
        "    const float* d_output, const float* weight, float* d_input,\n",
        "    int batch, int in_channels, int out_channels,\n",
        "    int H_in, int W_in, int H_out, int W_out,\n",
        "    int kH, int kW, int stride, int padding) {\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total = batch * in_channels * H_in * W_in;\n",
        "    if (idx < total) {\n",
        "        int temp = idx;\n",
        "        int x = temp % W_in;\n",
        "        temp /= W_in;\n",
        "        int y = temp % H_in;\n",
        "        temp /= H_in;\n",
        "        int ic = temp % in_channels;\n",
        "        int n = temp / in_channels;\n",
        "\n",
        "        float sum = 0.0f;\n",
        "        for (int oc = 0; oc < out_channels; oc++) {\n",
        "            for (int ky = 0; ky < kH; ky++) {\n",
        "                for (int kx = 0; kx < kW; kx++) {\n",
        "                    int o_y = y * stride - padding + ky;\n",
        "                    int o_x = x * stride - padding + kx;\n",
        "                    if (o_y >= 0 && o_y < H_out && o_x >= 0 && o_x < W_out) {\n",
        "                        int d_output_idx = ((n * out_channels + oc) * H_out + o_y) * W_out + o_x;\n",
        "                        int weight_idx = ((ic * out_channels + oc) * kH + ky) * kW + kx;\n",
        "                        sum += d_output[d_output_idx] * weight[weight_idx];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        d_input[idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void conv_transpose2d_backward_weight_kernel(\n",
        "    const float* input, const float* d_output, float* d_weight,\n",
        "    int batch, int in_channels, int out_channels,\n",
        "    int H_in, int W_in, int H_out, int W_out,\n",
        "    int kH, int kW, int stride, int padding) {\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total = in_channels * out_channels * kH * kW;\n",
        "    if (idx < total) {\n",
        "        int temp = idx;\n",
        "        int kx = temp % kW;\n",
        "        temp /= kW;\n",
        "        int ky = temp % kH;\n",
        "        temp /= kH;\n",
        "        int oc = temp % out_channels;\n",
        "        int ic = temp / out_channels;\n",
        "\n",
        "        float sum = 0.0f;\n",
        "        for (int n = 0; n < batch; n++) {\n",
        "            for (int y = 0; y < H_out; y++) {\n",
        "                for (int x = 0; x < W_out; x++) {\n",
        "                    int i_y = y * stride - padding + ky;\n",
        "                    int i_x = x * stride - padding + kx;\n",
        "                    if (i_y >= 0 && i_y < H_in && i_x >= 0 && i_x < W_in) {\n",
        "                        int input_idx = ((n * in_channels + ic) * H_in + i_y) * W_in + i_x;\n",
        "                        int d_output_idx = ((n * out_channels + oc) * H_out + y) * W_out + x;\n",
        "                        sum += input[input_idx] * d_output[d_output_idx];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        d_weight[idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_conv_transpose2d_backward(\n",
        "        const float* d_output, const float* input, const float* weight,\n",
        "        float* d_input, float* d_weight,\n",
        "        int batch, int in_channels, int out_channels,\n",
        "        int H_in, int W_in, int H_out, int W_out,\n",
        "        int kH, int kW, int stride, int padding) {\n",
        "\n",
        "        int total_input = batch * in_channels * H_in * W_in;\n",
        "        int total_weight = in_channels * out_channels * kH * kW;\n",
        "\n",
        "        int blockSize = 256;\n",
        "        int gridSize_input = (total_input + blockSize - 1) / blockSize;\n",
        "        int gridSize_weight = (total_weight + blockSize - 1) / blockSize;\n",
        "\n",
        "        conv_transpose2d_backward_input_kernel<<<gridSize_input, blockSize>>>(\n",
        "            d_output, weight, d_input, batch, in_channels, out_channels,\n",
        "            H_in, W_in, H_out, W_out, kH, kW, stride, padding);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        conv_transpose2d_backward_weight_kernel<<<gridSize_weight, blockSize>>>(\n",
        "            input, d_output, d_weight, batch, in_channels, out_channels,\n",
        "            H_in, W_in, H_out, W_out, kH, kW, stride, padding);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void conv2d_backward_input_kernel(\n",
        "    const float* d_output, const float* weight, float* d_input,\n",
        "    int batch, int in_channels, int out_channels,\n",
        "    int H_in, int W_in, int H_out, int W_out,\n",
        "    int kH, int kW, int stride, int padding) {\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total = batch * in_channels * H_in * W_in;\n",
        "    if (idx < total) {\n",
        "        int temp = idx;\n",
        "        int x = temp % W_in;\n",
        "        temp /= W_in;\n",
        "        int y = temp % H_in;\n",
        "        temp /= H_in;\n",
        "        int ic = temp % in_channels;\n",
        "        int n = temp / in_channels;\n",
        "\n",
        "        float sum = 0.0f;\n",
        "        for (int oc = 0; oc < out_channels; oc++) {\n",
        "            for (int ky = 0; ky < kH; ky++) {\n",
        "                for (int kx = 0; kx < kW; kx++) {\n",
        "                    int o_y = (y + padding - ky) / stride;\n",
        "                    int o_x = (x + padding - kx) / stride;\n",
        "                    if ((y + padding - ky) % stride == 0 && (x + padding - kx) % stride == 0 &&\n",
        "                        o_y >= 0 && o_y < H_out && o_x >= 0 && o_x < W_out) {\n",
        "                        int d_output_idx = ((n * out_channels + oc) * H_out + o_y) * W_out + o_x;\n",
        "                        int weight_idx = ((oc * in_channels + ic) * kH + ky) * kW + kx;\n",
        "                        sum += d_output[d_output_idx] * weight[weight_idx];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        d_input[idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void conv2d_backward_weight_kernel(\n",
        "    const float* input, const float* d_output, float* d_weight,\n",
        "    int batch, int in_channels, int out_channels,\n",
        "    int H_in, int W_in, int H_out, int W_out,\n",
        "    int kH, int kW, int stride, int padding) {\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total = out_channels * in_channels * kH * kW;\n",
        "    if (idx < total) {\n",
        "        int temp = idx;\n",
        "        int kx = temp % kW;\n",
        "        temp /= kW;\n",
        "        int ky = temp % kH;\n",
        "        temp /= kH;\n",
        "        int ic = temp % in_channels;\n",
        "        int oc = temp / in_channels;\n",
        "\n",
        "        float sum = 0.0f;\n",
        "        for (int n = 0; n < batch; n++) {\n",
        "            for (int y = 0; y < H_out; y++) {\n",
        "                for (int x = 0; x < W_out; x++) {\n",
        "                    int i_y = y * stride - padding + ky;\n",
        "                    int i_x = x * stride - padding + kx;\n",
        "                    if (i_y >= 0 && i_y < H_in && i_x >= 0 && i_x < W_in) {\n",
        "                        int input_idx = ((n * in_channels + ic) * H_in + i_y) * W_in + i_x;\n",
        "                        int d_output_idx = ((n * out_channels + oc) * H_out + y) * W_out + x;\n",
        "                        sum += input[input_idx] * d_output[d_output_idx];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        d_weight[idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_conv2d_backward(\n",
        "        const float* d_output, const float* input, const float* weight,\n",
        "        float* d_input, float* d_weight,\n",
        "        int batch, int in_channels, int out_channels,\n",
        "        int H_in, int W_in, int H_out, int W_out,\n",
        "        int kH, int kW, int stride, int padding) {\n",
        "\n",
        "        int total_input = batch * in_channels * H_in * W_in;\n",
        "        int total_weight = out_channels * in_channels * kH * kW;\n",
        "\n",
        "        int blockSize = 256;\n",
        "        int gridSize_input = (total_input + blockSize - 1) / blockSize;\n",
        "        int gridSize_weight = (total_weight + blockSize - 1) / blockSize;\n",
        "\n",
        "        conv2d_backward_input_kernel<<<gridSize_input, blockSize>>>(\n",
        "            d_output, weight, d_input, batch, in_channels, out_channels,\n",
        "            H_in, W_in, H_out, W_out, kH, kW, stride, padding);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        conv2d_backward_weight_kernel<<<gridSize_weight, blockSize>>>(\n",
        "            input, d_output, d_weight, batch, in_channels, out_channels,\n",
        "            H_in, W_in, H_out, W_out, kH, kW, stride, padding);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void batchnorm2d_backward_kernel(\n",
        "    const float* grad_output, const float* input, const float* mean, const float* var,\n",
        "    const float* gamma, float* d_input, float* d_gamma, float* d_beta,\n",
        "    int N, int C, int H, int W, float eps) {\n",
        "\n",
        "    int c = blockIdx.x;\n",
        "    int channel_size = N * H * W;\n",
        "\n",
        "    __shared__ float shared_dgamma[256];\n",
        "    __shared__ float shared_dbeta[256];\n",
        "\n",
        "    float dgamma = 0.0f;\n",
        "    float dbeta = 0.0f;\n",
        "    float inv_std = rsqrtf(var[c] + eps);\n",
        "\n",
        "    for (int i = threadIdx.x; i < channel_size; i += blockDim.x) {\n",
        "        int n = i / (H * W);\n",
        "        int rem = i % (H * W);\n",
        "        int h = rem / W;\n",
        "        int w = rem % W;\n",
        "        int idx = ((n * C + c) * H + h) * W + w;\n",
        "        float x_hat = (input[idx] - mean[c]) * inv_std;\n",
        "        dgamma += grad_output[idx] * x_hat;\n",
        "        dbeta += grad_output[idx];\n",
        "    }\n",
        "\n",
        "    shared_dgamma[threadIdx.x] = dgamma;\n",
        "    shared_dbeta[threadIdx.x] = dbeta;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n",
        "        if (threadIdx.x < stride) {\n",
        "            shared_dgamma[threadIdx.x] += shared_dgamma[threadIdx.x + stride];\n",
        "            shared_dbeta[threadIdx.x] += shared_dbeta[threadIdx.x + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (threadIdx.x == 0) {\n",
        "        atomicAdd(&d_gamma[c], shared_dgamma[0]);\n",
        "        atomicAdd(&d_beta[c], shared_dbeta[0]);\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int i = threadIdx.x; i < channel_size; i += blockDim.x) {\n",
        "        int n = i / (H * W);\n",
        "        int rem = i % (H * W);\n",
        "        int h = rem / W;\n",
        "        int w = rem % W;\n",
        "        int idx = ((n * C + c) * H + h) * W + w;\n",
        "\n",
        "        float x_hat = (input[idx] - mean[c]) * inv_std;\n",
        "        float d_input_val = (grad_output[idx] - dbeta / channel_size - x_hat * dgamma / channel_size) * gamma[c] * inv_std;\n",
        "        d_input[idx] = d_input_val;\n",
        "    }\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_batchnorm2d_backward(\n",
        "        const float* grad_output, const float* input, const float* mean, const float* var,\n",
        "        const float* gamma, float* d_input, float* d_gamma, float* d_beta,\n",
        "        int N, int C, int H, int W, float eps) {\n",
        "\n",
        "        int blockSize = 256;\n",
        "        int gridSize = C;\n",
        "\n",
        "        batchnorm2d_backward_kernel<<<gridSize, blockSize>>>(\n",
        "            grad_output, input, mean, var, gamma, d_input, d_gamma, d_beta,\n",
        "            N, C, H, W, eps);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void leaky_relu_kernel(const float* input, float* output, int n, float alpha) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        float x = input[idx];\n",
        "        output[idx] = (x > 0.0f) ? x : alpha * x;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void leaky_relu_backward_kernel(const float* input, const float* grad_output, float* grad_input, int n, float alpha) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        grad_input[idx] = (input[idx] > 0.0f) ? grad_output[idx] : alpha * grad_output[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_leaky_relu(const float* input, float* output, int n, float alpha) {\n",
        "        int blockSize = 256;\n",
        "        int gridSize = (n + blockSize - 1) / blockSize;\n",
        "        leaky_relu_kernel<<<gridSize, blockSize>>>(input, output, n, alpha);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    void launch_leaky_relu_backward(const float* input, const float* grad_output, float* grad_input, int n, float alpha) {\n",
        "        int blockSize = 256;\n",
        "        int gridSize = (n + blockSize - 1) / blockSize;\n",
        "        leaky_relu_backward_kernel<<<gridSize, blockSize>>>(\n",
        "            input, grad_output, grad_input, n, alpha\n",
        "        );\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void sigmoid_kernel(const float* input, float* output, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        output[idx] = 1.0f / (1.0f + expf(-input[idx]));\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void sigmoid_backward_kernel(const float* output, const float* grad_output, float* grad_input, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        float sigmoid_val = output[idx];\n",
        "        grad_input[idx] = grad_output[idx] * sigmoid_val * (1.0f - sigmoid_val);\n",
        "    }\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_sigmoid(const float* input, float* output, int n) {\n",
        "        int blockSize = 256;\n",
        "        int gridSize = (n + blockSize - 1) / blockSize;\n",
        "        sigmoid_kernel<<<gridSize, blockSize>>>(input, output, n);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    void launch_sigmoid_backward(const float* output, const float* grad_output, float* grad_input, int n) {\n",
        "        int blockSize = 256;\n",
        "        int gridSize = (n + blockSize - 1) / blockSize;\n",
        "        sigmoid_backward_kernel<<<gridSize, blockSize>>>(output, grad_output, grad_input, n);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void tanh_kernel(const float* input, float* output, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        output[idx] = tanhf(input[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void tanh_backward_kernel(const float* output, const float* grad_output, float* grad_input, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        float tanh_val = output[idx];\n",
        "        grad_input[idx] = grad_output[idx] * (1.0f - tanh_val * tanh_val);\n",
        "    }\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_tanh(const float* input, float* output, int n) {\n",
        "        int blockSize = 256;\n",
        "        int gridSize = (n + blockSize - 1) / blockSize;\n",
        "        tanh_kernel<<<gridSize, blockSize>>>(input, output, n);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    void launch_tanh_backward(const float* output, const float* grad_output, float* grad_input, int n) {\n",
        "        int blockSize = 256;\n",
        "        int gridSize = (n + blockSize - 1) / blockSize;\n",
        "        tanh_backward_kernel<<<gridSize, blockSize>>>(output, grad_output, grad_input, n);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// OPTIMIZED CODE GOES HERE\n",
        "\n",
        "__global__ void conv_transpose2d_optimized_kernel(\n",
        "    const float* __restrict__ input, const float* __restrict__ weight, float* output,\n",
        "    int batch, int in_channels, int out_channels,\n",
        "    int H_in, int W_in, int H_out, int W_out,\n",
        "    int kH, int kW, int stride, int padding) {\n",
        "\n",
        "    extern __shared__ float shared_weight[];\n",
        "\n",
        "    int oc = blockIdx.x;\n",
        "    int n = blockIdx.y;\n",
        "    int y = threadIdx.y + blockIdx.z * blockDim.y;\n",
        "    int x = threadIdx.x + blockIdx.z * blockDim.x;\n",
        "\n",
        "    if (y >= H_out || x >= W_out) return;\n",
        "\n",
        "    // Load weights into shared memory (one-time load per block)\n",
        "    if (threadIdx.y < kH && threadIdx.x < kW) {\n",
        "        for (int ic = 0; ic < in_channels; ic++) {\n",
        "            shared_weight[(ic * out_channels + oc) * kH * kW + threadIdx.y * kW + threadIdx.x] =\n",
        "                weight[(ic * out_channels + oc) * kH * kW + threadIdx.y * kW + threadIdx.x];\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    float sum = 0.0f;\n",
        "\n",
        "    for (int ic = 0; ic < in_channels; ic++) {\n",
        "        for (int ky = 0; ky < kH; ky++) {\n",
        "            for (int kx = 0; kx < kW; kx++) {\n",
        "                int i_y = y + padding - ky;\n",
        "                int i_x = x + padding - kx;\n",
        "\n",
        "                if (i_y % stride == 0 && i_x % stride == 0) {\n",
        "                    i_y /= stride;\n",
        "                    i_x /= stride;\n",
        "\n",
        "                    if (i_y >= 0 && i_y < H_in && i_x >= 0 && i_x < W_in) {\n",
        "                        int input_idx = ((n * in_channels + ic) * H_in + i_y) * W_in + i_x;\n",
        "                        int weight_idx = (ic * out_channels + oc) * kH * kW + ky * kW + kx;\n",
        "                        sum += input[input_idx] * shared_weight[weight_idx];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Store result in output tensor\n",
        "    output[((n * out_channels + oc) * H_out + y) * W_out + x] = sum;\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_conv_transpose2d_optimized(\n",
        "        const float* input, const float* weight, float* output,\n",
        "        int batch, int in_channels, int out_channels,\n",
        "        int H_in, int W_in, int H_out, int W_out,\n",
        "        int kH, int kW, int stride, int padding) {\n",
        "\n",
        "        dim3 blockDim(16, 16);  // 16x16 threads per block\n",
        "        dim3 gridDim(out_channels, batch, (H_out * W_out + blockDim.x * blockDim.y - 1) / (blockDim.x * blockDim.y));\n",
        "\n",
        "        size_t sharedMemSize = in_channels * out_channels * kH * kW * sizeof(float); // Shared memory size\n",
        "\n",
        "        conv_transpose2d_optimized_kernel<<<gridDim, blockDim, sharedMemSize>>>(\n",
        "            input, weight, output, batch, in_channels, out_channels,\n",
        "            H_in, W_in, H_out, W_out, kH, kW, stride, padding);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void conv_transpose2d_backward_input_kernel_optimized(\n",
        "    const float* __restrict__ d_output, const float* __restrict__ weight, float* d_input,\n",
        "    int batch, int in_channels, int out_channels,\n",
        "    int H_in, int W_in, int H_out, int W_out,\n",
        "    int kH, int kW, int stride, int padding) {\n",
        "\n",
        "    extern __shared__ float shared_weight[];\n",
        "\n",
        "    int ic = blockIdx.x;  // Each block computes an input channel\n",
        "    int n = blockIdx.y;   // Each block computes a batch sample\n",
        "    int y = threadIdx.y + blockIdx.z * blockDim.y;\n",
        "    int x = threadIdx.x + blockIdx.z * blockDim.x;\n",
        "\n",
        "    if (y >= H_in || x >= W_in) return;\n",
        "\n",
        "    if (threadIdx.y < kH && threadIdx.x < kW) {\n",
        "        for (int oc = 0; oc < out_channels; oc++) {\n",
        "            shared_weight[(oc * in_channels + ic) * kH * kW + threadIdx.y * kW + threadIdx.x] =\n",
        "                weight[(oc * in_channels + ic) * kH * kW + threadIdx.y * kW + threadIdx.x];\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    float sum = 0.0f;\n",
        "\n",
        "    for (int oc = 0; oc < out_channels; oc++) {\n",
        "        for (int ky = 0; ky < kH; ky++) {\n",
        "            for (int kx = 0; kx < kW; kx++) {\n",
        "                int o_y = y * stride - padding + ky;\n",
        "                int o_x = x * stride - padding + kx;\n",
        "\n",
        "                if (o_y >= 0 && o_y < H_out && o_x >= 0 && o_x < W_out) {\n",
        "                    int d_output_idx = ((n * out_channels + oc) * H_out + o_y) * W_out + o_x;\n",
        "                    int weight_idx = (oc * in_channels + ic) * kH * kW + ky * kW + kx;\n",
        "                    sum += d_output[d_output_idx] * shared_weight[weight_idx];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    d_input[((n * in_channels + ic) * H_in + y) * W_in + x] = sum;\n",
        "}\n",
        "\n",
        "__global__ void conv_transpose2d_backward_weight_kernel_optimized(\n",
        "    const float* __restrict__ input, const float* __restrict__ d_output, float* d_weight,\n",
        "    int batch, int in_channels, int out_channels,\n",
        "    int H_in, int W_in, int H_out, int W_out,\n",
        "    int kH, int kW, int stride, int padding) {\n",
        "\n",
        "    int ic = blockIdx.x;\n",
        "    int oc = blockIdx.y;\n",
        "    int ky = threadIdx.y;\n",
        "    int kx = threadIdx.x;\n",
        "\n",
        "    if (ky >= kH || kx >= kW) return;\n",
        "\n",
        "    float sum = 0.0f;\n",
        "\n",
        "    for (int n = 0; n < batch; n++) {\n",
        "        for (int y = 0; y < H_out; y++) {\n",
        "            for (int x = 0; x < W_out; x++) {\n",
        "                int i_y = y * stride - padding + ky;\n",
        "                int i_x = x * stride - padding + kx;\n",
        "                if (i_y >= 0 && i_y < H_in && i_x >= 0 && i_x < W_in) {\n",
        "                    int input_idx = ((n * in_channels + ic) * H_in + i_y) * W_in + i_x;\n",
        "                    int d_output_idx = ((n * out_channels + oc) * H_out + y) * W_out + x;\n",
        "                    sum += input[input_idx] * d_output[d_output_idx];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    d_weight[(ic * out_channels + oc) * kH * kW + ky * kW + kx] = sum;\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_conv_transpose2d_backward_optimized(\n",
        "        const float* d_output, const float* input, const float* weight,\n",
        "        float* d_input, float* d_weight,\n",
        "        int batch, int in_channels, int out_channels,\n",
        "        int H_in, int W_in, int H_out, int W_out,\n",
        "        int kH, int kW, int stride, int padding) {\n",
        "\n",
        "        dim3 blockDim(16, 16);\n",
        "        dim3 gridDim_input(in_channels, batch, (H_in * W_in + blockDim.x * blockDim.y - 1) / (blockDim.x * blockDim.y));\n",
        "        dim3 gridDim_weight(in_channels, out_channels);\n",
        "\n",
        "        size_t sharedMemSize = in_channels * out_channels * kH * kW * sizeof(float);\n",
        "\n",
        "        conv_transpose2d_backward_input_kernel_optimized<<<gridDim_input, blockDim, sharedMemSize>>>(\n",
        "            d_output, weight, d_input, batch, in_channels, out_channels,\n",
        "            H_in, W_in, H_out, W_out, kH, kW, stride, padding);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        conv_transpose2d_backward_weight_kernel_optimized<<<gridDim_weight, dim3(kH, kW)>>>(\n",
        "            input, d_output, d_weight, batch, in_channels, out_channels,\n",
        "            H_in, W_in, H_out, W_out, kH, kW, stride, padding);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void conv2d_forward_optimized_kernel(\n",
        "    const float* __restrict__ input, const float* __restrict__ weight, float* output,\n",
        "    int batch, int in_channels, int out_channels,\n",
        "    int H_in, int W_in, int H_out, int W_out,\n",
        "    int kH, int kW, int stride, int padding) {\n",
        "\n",
        "    extern __shared__ float shared_weight[];\n",
        "\n",
        "    int oc = blockIdx.x;  // Each block computes an output channel\n",
        "    int n = blockIdx.y;   // Each block computes a batch sample\n",
        "    int y = threadIdx.y + blockIdx.z * blockDim.y;\n",
        "    int x = threadIdx.x + blockIdx.z * blockDim.x;\n",
        "\n",
        "    if (y >= H_out || x >= W_out) return; // Boundary check\n",
        "\n",
        "    // Load weights into shared memory (one-time load per block)\n",
        "    if (threadIdx.y < kH && threadIdx.x < kW) {\n",
        "        for (int ic = 0; ic < in_channels; ic++) {\n",
        "            shared_weight[(oc * in_channels + ic) * kH * kW + threadIdx.y * kW + threadIdx.x] =\n",
        "                weight[(oc * in_channels + ic) * kH * kW + threadIdx.y * kW + threadIdx.x];\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    float sum = 0.0f;\n",
        "\n",
        "    for (int ic = 0; ic < in_channels; ic++) {\n",
        "        for (int ky = 0; ky < kH; ky++) {\n",
        "            for (int kx = 0; kx < kW; kx++) {\n",
        "                int i_y = y * stride - padding + ky;\n",
        "                int i_x = x * stride - padding + kx;\n",
        "                if (i_y >= 0 && i_y < H_in && i_x >= 0 && i_x < W_in) {\n",
        "                    int input_idx = ((n * in_channels + ic) * H_in + i_y) * W_in + i_x;\n",
        "                    int weight_idx = (oc * in_channels + ic) * kH * kW + ky * kW + kx;\n",
        "                    sum += input[input_idx] * shared_weight[weight_idx];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    output[((n * out_channels + oc) * H_out + y) * W_out + x] = sum;\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_conv2d_forward_optimized(\n",
        "        const float* input, const float* weight, float* output,\n",
        "        int batch, int in_channels, int out_channels,\n",
        "        int H_in, int W_in, int H_out, int W_out,\n",
        "        int kH, int kW, int stride, int padding) {\n",
        "\n",
        "        dim3 blockDim(16, 16);\n",
        "        dim3 gridDim(out_channels, batch, (H_out * W_out + blockDim.x * blockDim.y - 1) / (blockDim.x * blockDim.y));\n",
        "\n",
        "        size_t sharedMemSize = out_channels * in_channels * kH * kW * sizeof(float);\n",
        "\n",
        "        conv2d_forward_optimized_kernel<<<gridDim, blockDim, sharedMemSize>>>(\n",
        "            input, weight, output, batch, in_channels, out_channels,\n",
        "            H_in, W_in, H_out, W_out, kH, kW, stride, padding);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void conv2d_backward_input_kernel_optimized(\n",
        "    const float* __restrict__ d_output, const float* __restrict__ weight, float* d_input,\n",
        "    int batch, int in_channels, int out_channels,\n",
        "    int H_in, int W_in, int H_out, int W_out,\n",
        "    int kH, int kW, int stride, int padding) {\n",
        "\n",
        "    extern __shared__ float shared_weight[];\n",
        "\n",
        "    int ic = blockIdx.x;\n",
        "    int n = blockIdx.y;\n",
        "    int y = threadIdx.y + blockIdx.z * blockDim.y;\n",
        "    int x = threadIdx.x + blockIdx.z * blockDim.x;\n",
        "\n",
        "    if (y >= H_in || x >= W_in) return;\n",
        "\n",
        "    if (threadIdx.y < kH && threadIdx.x < kW) {\n",
        "        for (int oc = 0; oc < out_channels; oc++) {\n",
        "            shared_weight[(oc * in_channels + ic) * kH * kW + threadIdx.y * kW + threadIdx.x] =\n",
        "                weight[(oc * in_channels + ic) * kH * kW + threadIdx.y * kW + threadIdx.x];\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    float sum = 0.0f;\n",
        "\n",
        "    for (int oc = 0; oc < out_channels; oc++) {\n",
        "        for (int ky = 0; ky < kH; ky++) {\n",
        "            for (int kx = 0; kx < kW; kx++) {\n",
        "                int o_y = y * stride - padding + ky;\n",
        "                int o_x = x * stride - padding + kx;\n",
        "                if (o_y >= 0 && o_y < H_out && o_x >= 0 && o_x < W_out) {\n",
        "                    int d_output_idx = ((n * out_channels + oc) * H_out + o_y) * W_out + o_x;\n",
        "                    int weight_idx = (oc * in_channels + ic) * kH * kW + ky * kW + kx;\n",
        "                    sum += d_output[d_output_idx] * shared_weight[weight_idx];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    d_input[((n * in_channels + ic) * H_in + y) * W_in + x] = sum;\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_conv2d_backward_optimized(\n",
        "        const float* d_output, const float* input, const float* weight,\n",
        "        float* d_input, float* d_weight,\n",
        "        int batch, int in_channels, int out_channels,\n",
        "        int H_in, int W_in, int H_out, int W_out,\n",
        "        int kH, int kW, int stride, int padding) {\n",
        "\n",
        "        dim3 blockDim(16, 16);\n",
        "        dim3 gridDim(in_channels, batch, (H_in * W_in + blockDim.x * blockDim.y - 1) / (blockDim.x * blockDim.y));\n",
        "\n",
        "        size_t sharedMemSize = out_channels * in_channels * kH * kW * sizeof(float);\n",
        "\n",
        "        conv2d_backward_input_kernel_optimized<<<gridDim, blockDim, sharedMemSize>>>(\n",
        "            d_output, weight, d_input, batch, in_channels, out_channels,\n",
        "            H_in, W_in, H_out, W_out, kH, kW, stride, padding);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void batchnorm_forward_compute_mean_var(\n",
        "    const float* __restrict__ input, float* mean, float* var, int N, int C, int H, int W) {\n",
        "\n",
        "    int c = blockIdx.x;  // One block per channel\n",
        "    int tid = threadIdx.x;\n",
        "    int num_pixels = N * H * W;\n",
        "\n",
        "    extern __shared__ float shared_sum[];\n",
        "    shared_sum[tid] = 0.0f;\n",
        "    __syncthreads();\n",
        "\n",
        "    float sum = 0.0f;\n",
        "    for (int i = tid; i < num_pixels; i += blockDim.x) {\n",
        "        int n = i / (H * W);\n",
        "        int hw = i % (H * W);\n",
        "        int index = ((n * C + c) * H + hw / W) * W + hw % W;\n",
        "        sum += input[index];\n",
        "    }\n",
        "\n",
        "    shared_sum[tid] = sum;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Reduce within the block\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n",
        "        if (tid < stride) {\n",
        "            shared_sum[tid] += shared_sum[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        mean[c] = shared_sum[0] / num_pixels;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Compute variance\n",
        "    float sum_sq = 0.0f;\n",
        "    for (int i = tid; i < num_pixels; i += blockDim.x) {\n",
        "        int n = i / (H * W);\n",
        "        int hw = i % (H * W);\n",
        "        int index = ((n * C + c) * H + hw / W) * W + hw % W;\n",
        "        float diff = input[index] - mean[c];\n",
        "        sum_sq += diff * diff;\n",
        "    }\n",
        "\n",
        "    shared_sum[tid] = sum_sq;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n",
        "        if (tid < stride) {\n",
        "            shared_sum[tid] += shared_sum[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        var[c] = shared_sum[0] / num_pixels;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void batchnorm_forward_apply(\n",
        "    const float* __restrict__ input, float* output, const float* mean, const float* var,\n",
        "    const float* gamma, const float* beta, int N, int C, int H, int W, float eps) {\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total = N * C * H * W;\n",
        "    if (idx < total) {\n",
        "        int n = idx / (C * H * W);\n",
        "        int rem = idx % (C * H * W);\n",
        "        int c = rem / (H * W);\n",
        "        int h = (rem % (H * W)) / W;\n",
        "        int w = (rem % (H * W)) % W;\n",
        "        int index = ((n * C + c) * H + h) * W + w;\n",
        "        float norm = (input[index] - mean[c]) / sqrtf(var[c] + eps);\n",
        "        output[index] = gamma[c] * norm + beta[c];\n",
        "    }\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_batchnorm_forward_optimized(\n",
        "        const float* input, float* output, const float* gamma, const float* beta,\n",
        "        int N, int C, int H, int W, float eps) {\n",
        "\n",
        "        int blockSize = 256;\n",
        "        int gridSize = C;\n",
        "        size_t sharedMemSize = blockSize * sizeof(float);\n",
        "\n",
        "        float *mean, *var;\n",
        "        cudaMalloc(&mean, C * sizeof(float));\n",
        "        cudaMalloc(&var, C * sizeof(float));\n",
        "\n",
        "        batchnorm_forward_compute_mean_var<<<gridSize, blockSize, sharedMemSize>>>(\n",
        "            input, mean, var, N, C, H, W);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        int total = N * C * H * W;\n",
        "        batchnorm_forward_apply<<<(total + blockSize - 1) / blockSize, blockSize>>>(\n",
        "            input, output, mean, var, gamma, beta, N, C, H, W, eps);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        cudaFree(mean);\n",
        "        cudaFree(var);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void batchnorm_backward_kernel(\n",
        "    const float* __restrict__ grad_output, const float* mean, const float* var,\n",
        "    const float* gamma, float* grad_input, int N, int C, int H, int W, float eps) {\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total = N * C * H * W;\n",
        "    if (idx < total) {\n",
        "        int n = idx / (C * H * W);\n",
        "        int rem = idx % (C * H * W);\n",
        "        int c = rem / (H * W);\n",
        "        int h = (rem % (H * W)) / W;\n",
        "        int w = (rem % (H * W)) % W;\n",
        "        int index = ((n * C + c) * H + h) * W + w;\n",
        "\n",
        "        float inv_std = 1.0f / sqrtf(var[c] + eps);\n",
        "        grad_input[index] = grad_output[index] * gamma[c] * inv_std;\n",
        "    }\n",
        "}\n",
        "\n",
        "extern \"C\" {\n",
        "    void launch_batchnorm_backward_optimized(\n",
        "        const float* grad_output, const float* mean, const float* var,\n",
        "        const float* gamma, float* grad_input, int N, int C, int H, int W, float eps) {\n",
        "\n",
        "        int total = N * C * H * W;\n",
        "        int blockSize = 256;\n",
        "        batchnorm_backward_kernel<<<(total + blockSize - 1) / blockSize, blockSize>>>(\n",
        "            grad_output, mean, var, gamma, grad_input, N, C, H, W, eps);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1K0zFCGzRSq",
        "outputId": "5384e368-81c9-47cf-9ee5-fa898e7c60a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda_kernels_optimized.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -shared -o cuda_kernels_optimized.so -Xcompiler -fPIC cuda_kernels_optimized.cu"
      ],
      "metadata": {
        "id": "YISRR-ULYYw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25ae63ab-5b6e-4c84-afd0-f3c55eb42498"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mcuda_kernels_optimized.cu(168)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"channel_size\"\u001b[0m was declared but never referenced\n",
            "          int channel_size = N * H * W;\n",
            "              ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh cuda_kernels_optimized.so"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utZ3ujnlZnQb",
        "outputId": "235d16fd-bc16-4c89-bf93-32951d74893d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rwxr-xr-x 1 root root 1.2M Mar 17 04:43 cuda_kernels_optimized.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ctypes\n",
        "import torch\n",
        "\n",
        "cuda_lib = ctypes.cdll.LoadLibrary('./cuda_kernels_optimized.so')\n",
        "\n",
        "cuda_lib.launch_double_elements.argtypes = [ctypes.c_void_p, ctypes.c_int]\n",
        "\n",
        "def double_elements(tensor):\n",
        "    if not tensor.is_cuda:\n",
        "        raise ValueError(\"Tensor must be on CUDA\")\n",
        "    if not tensor.is_contiguous():\n",
        "        tensor = tensor.contiguous()\n",
        "    ptr = tensor.data_ptr()\n",
        "    n = tensor.numel()\n",
        "    cuda_lib.launch_double_elements(ctypes.c_void_p(ptr), ctypes.c_int(n))\n",
        "\n",
        "\n",
        "# Set argument types for the kernel launcher.\n",
        "cuda_lib.launch_conv_transpose_general.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int\n",
        "]\n",
        "\n",
        "cuda_lib.launch_double_elements_backward.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int]\n",
        "\n",
        "def double_elements_backward(grad_output):\n",
        "    if not grad_output.is_cuda:\n",
        "        raise ValueError(\"Tensor must be on CUDA\")\n",
        "    if not grad_output.is_contiguous():\n",
        "        grad_output = grad_output.contiguous()\n",
        "\n",
        "    grad_input = torch.empty_like(grad_output)\n",
        "\n",
        "    cuda_lib.launch_double_elements_backward(\n",
        "        ctypes.c_void_p(grad_output.data_ptr()),\n",
        "        ctypes.c_void_p(grad_input.data_ptr()),\n",
        "        ctypes.c_int(grad_output.numel())\n",
        "    )\n",
        "\n",
        "    return grad_input\n",
        "\n",
        "\n",
        "def conv_transpose_general_forward(input, weight, stride, padding):\n",
        "    input = input.contiguous()\n",
        "    weight = weight.contiguous()\n",
        "    batch, in_channels, H_in, W_in = input.shape\n",
        "    _, out_channels, kH, kW = weight.shape\n",
        "    H_out = (H_in - 1) * stride - 2 * padding + kH\n",
        "    W_out = (W_in - 1) * stride - 2 * padding + kW\n",
        "    output_shape = (batch, out_channels, H_out, W_out)\n",
        "    output = torch.empty(output_shape, device=input.device, dtype=input.dtype)\n",
        "\n",
        "    cuda_lib.launch_conv_transpose_general(\n",
        "        ctypes.c_void_p(int(input.data_ptr())),\n",
        "        ctypes.c_void_p(int(weight.data_ptr())),\n",
        "        ctypes.c_void_p(int(output.data_ptr())),\n",
        "        ctypes.c_int(batch),\n",
        "        ctypes.c_int(in_channels),\n",
        "        ctypes.c_int(out_channels),\n",
        "        ctypes.c_int(H_in),\n",
        "        ctypes.c_int(W_in),\n",
        "        ctypes.c_int(H_out),\n",
        "        ctypes.c_int(W_out),\n",
        "        ctypes.c_int(kH),\n",
        "        ctypes.c_int(kW),\n",
        "        ctypes.c_int(stride),\n",
        "        ctypes.c_int(padding)\n",
        "    )\n",
        "    return output\n",
        "\n",
        "cuda_lib.launch_batchnorm.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_float\n",
        "]\n",
        "\n",
        "def batchnorm_forward(input, gamma, beta, eps, N, C, H, W):\n",
        "    input = input.contiguous()\n",
        "    output = torch.empty_like(input)\n",
        "    cuda_lib.launch_batchnorm(ctypes.c_void_p(int(input.data_ptr())),\n",
        "                                 ctypes.c_void_p(int(output.data_ptr())),\n",
        "                                 ctypes.c_void_p(int(gamma.data_ptr())),\n",
        "                                 ctypes.c_void_p(int(beta.data_ptr())),\n",
        "                                 ctypes.c_int(N),\n",
        "                                 ctypes.c_int(C),\n",
        "                                 ctypes.c_int(H),\n",
        "                                 ctypes.c_int(W),\n",
        "                                 ctypes.c_float(eps))\n",
        "    return output\n",
        "\n",
        "cuda_lib.launch_batchnorm2d_backward.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_float\n",
        "]\n",
        "\n",
        "def batchnorm2d_backward(grad_output, input, mean, var, gamma, eps):\n",
        "    input = input.contiguous()\n",
        "    grad_output = grad_output.contiguous()\n",
        "    gamma = gamma.contiguous()\n",
        "    mean = mean.contiguous()\n",
        "    var = var.contiguous()\n",
        "\n",
        "    N, C, H, W = input.shape\n",
        "    d_input = torch.zeros_like(input)\n",
        "    d_gamma = torch.zeros_like(gamma)\n",
        "    d_beta = torch.zeros_like(gamma)\n",
        "\n",
        "    cuda_lib.launch_batchnorm2d_backward(\n",
        "        ctypes.c_void_p(grad_output.data_ptr()),\n",
        "        ctypes.c_void_p(input.data_ptr()),\n",
        "        ctypes.c_void_p(mean.data_ptr()),\n",
        "        ctypes.c_void_p(var.data_ptr()),\n",
        "        ctypes.c_void_p(gamma.data_ptr()),\n",
        "        ctypes.c_void_p(d_input.data_ptr()),\n",
        "        ctypes.c_void_p(d_gamma.data_ptr()),\n",
        "        ctypes.c_void_p(d_beta.data_ptr()),\n",
        "        ctypes.c_int(N), ctypes.c_int(C), ctypes.c_int(H), ctypes.c_int(W),\n",
        "        ctypes.c_float(eps)\n",
        "    )\n",
        "\n",
        "    return d_input, d_gamma, d_beta\n",
        "\n",
        "\n",
        "cuda_lib.launch_conv2d.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int\n",
        "]\n",
        "\n",
        "def conv2d_forward(input, weight, stride, padding):\n",
        "    input = input.contiguous()\n",
        "    weight = weight.contiguous()\n",
        "    batch, in_channels, H_in, W_in = input.shape\n",
        "    out_channels, _, kH, kW = weight.shape\n",
        "    H_out = (H_in + 2 * padding - kH) // stride + 1\n",
        "    W_out = (W_in + 2 * padding - kW) // stride + 1\n",
        "    output_shape = (batch, out_channels, H_out, W_out)\n",
        "    output = torch.empty(output_shape, device=input.device, dtype=input.dtype)\n",
        "    cuda_lib.launch_conv2d(\n",
        "        ctypes.c_void_p(int(input.data_ptr())),\n",
        "        ctypes.c_void_p(int(weight.data_ptr())),\n",
        "        ctypes.c_void_p(int(output.data_ptr())),\n",
        "        ctypes.c_int(batch),\n",
        "        ctypes.c_int(in_channels),\n",
        "        ctypes.c_int(out_channels),\n",
        "        ctypes.c_int(H_in),\n",
        "        ctypes.c_int(W_in),\n",
        "        ctypes.c_int(H_out),\n",
        "        ctypes.c_int(W_out),\n",
        "        ctypes.c_int(kH),\n",
        "        ctypes.c_int(kW),\n",
        "        ctypes.c_int(stride),\n",
        "        ctypes.c_int(padding)\n",
        "    )\n",
        "    return output\n",
        "\n",
        "cuda_lib.launch_conv_transpose2d_backward.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int\n",
        "]\n",
        "\n",
        "def conv_transpose2d_backward(d_output, input, weight, stride, padding):\n",
        "    input = input.contiguous()\n",
        "    weight = weight.contiguous()\n",
        "    d_output = d_output.contiguous()\n",
        "\n",
        "    batch, in_channels, H_in, W_in = input.shape\n",
        "    _, out_channels, kH, kW = weight.shape\n",
        "    H_out = (H_in - 1) * stride - 2 * padding + kH\n",
        "    W_out = (W_in - 1) * stride - 2 * padding + kW\n",
        "\n",
        "    d_input = torch.zeros_like(input)\n",
        "    d_weight = torch.zeros_like(weight)\n",
        "\n",
        "    cuda_lib.launch_conv_transpose2d_backward(\n",
        "        ctypes.c_void_p(d_output.data_ptr()),\n",
        "        ctypes.c_void_p(input.data_ptr()),\n",
        "        ctypes.c_void_p(weight.data_ptr()),\n",
        "        ctypes.c_void_p(d_input.data_ptr()),\n",
        "        ctypes.c_void_p(d_weight.data_ptr()),\n",
        "        ctypes.c_int(batch), ctypes.c_int(in_channels), ctypes.c_int(out_channels),\n",
        "        ctypes.c_int(H_in), ctypes.c_int(W_in),\n",
        "        ctypes.c_int(H_out), ctypes.c_int(W_out),\n",
        "        ctypes.c_int(kH), ctypes.c_int(kW),\n",
        "        ctypes.c_int(stride), ctypes.c_int(padding)\n",
        "    )\n",
        "\n",
        "    return d_input, d_weight\n",
        "\n",
        "cuda_lib.launch_conv2d_backward.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int\n",
        "]\n",
        "\n",
        "def conv2d_backward(d_output, input, weight, stride, padding):\n",
        "    input = input.contiguous()\n",
        "    weight = weight.contiguous()\n",
        "    d_output = d_output.contiguous()\n",
        "\n",
        "    batch, in_channels, H_in, W_in = input.shape\n",
        "    out_channels, _, kH, kW = weight.shape\n",
        "    H_out = (H_in + 2 * padding - kH) // stride + 1\n",
        "    W_out = (W_in + 2 * padding - kW) // stride + 1\n",
        "\n",
        "    d_input = torch.zeros_like(input)\n",
        "    d_weight = torch.zeros_like(weight)\n",
        "\n",
        "    cuda_lib.launch_conv2d_backward(\n",
        "        ctypes.c_void_p(d_output.data_ptr()),\n",
        "        ctypes.c_void_p(input.data_ptr()),\n",
        "        ctypes.c_void_p(weight.data_ptr()),\n",
        "        ctypes.c_void_p(d_input.data_ptr()),\n",
        "        ctypes.c_void_p(d_weight.data_ptr()),\n",
        "        ctypes.c_int(batch), ctypes.c_int(in_channels), ctypes.c_int(out_channels),\n",
        "        ctypes.c_int(H_in), ctypes.c_int(W_in),\n",
        "        ctypes.c_int(H_out), ctypes.c_int(W_out),\n",
        "        ctypes.c_int(kH), ctypes.c_int(kW),\n",
        "        ctypes.c_int(stride), ctypes.c_int(padding)\n",
        "    )\n",
        "\n",
        "    return d_input, d_weight\n",
        "\n",
        "cuda_lib.launch_relu.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int]\n",
        "\n",
        "def relu_forward(input):\n",
        "    if not input.is_cuda:\n",
        "        raise ValueError(\"Tensor must be on CUDA\")\n",
        "    if not input.is_contiguous():\n",
        "        input = input.contiguous()\n",
        "    n = input.numel()\n",
        "    output = torch.empty_like(input)\n",
        "    cuda_lib.launch_relu(\n",
        "        ctypes.c_void_p(int(input.data_ptr())),\n",
        "        ctypes.c_void_p(int(output.data_ptr())),\n",
        "        ctypes.c_int(n)\n",
        "    )\n",
        "    return output\n",
        "\n",
        "cuda_lib.launch_relu_backward.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int\n",
        "]\n",
        "\n",
        "def relu_backward(input, grad_output):\n",
        "    if not input.is_cuda or not grad_output.is_cuda:\n",
        "        raise ValueError(\"Tensors must be on CUDA\")\n",
        "\n",
        "    if not input.is_contiguous():\n",
        "        input = input.contiguous()\n",
        "    if not grad_output.is_contiguous():\n",
        "        grad_output = grad_output.contiguous()\n",
        "\n",
        "    grad_input = torch.empty_like(grad_output)\n",
        "\n",
        "    cuda_lib.launch_relu_backward(\n",
        "        ctypes.c_void_p(input.data_ptr()),\n",
        "        ctypes.c_void_p(grad_output.data_ptr()),\n",
        "        ctypes.c_void_p(grad_input.data_ptr()),\n",
        "        ctypes.c_int(input.numel())\n",
        "    )\n",
        "\n",
        "    return grad_input\n",
        "\n",
        "cuda_lib.launch_leaky_relu.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.c_float\n",
        "]\n",
        "\n",
        "def leaky_relu_forward(input, alpha=0.01):\n",
        "    if not input.is_cuda:\n",
        "        raise ValueError(\"Tensor must be on CUDA\")\n",
        "    if not input.is_contiguous():\n",
        "        input = input.contiguous()\n",
        "\n",
        "    output = torch.empty_like(input)\n",
        "\n",
        "    cuda_lib.launch_leaky_relu(\n",
        "        ctypes.c_void_p(input.data_ptr()),\n",
        "        ctypes.c_void_p(output.data_ptr()),\n",
        "        ctypes.c_int(input.numel()),\n",
        "        ctypes.c_float(alpha)\n",
        "    )\n",
        "\n",
        "    return output\n",
        "\n",
        "cuda_lib.launch_leaky_relu_backward.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.c_float\n",
        "]\n",
        "\n",
        "def leaky_relu_backward(input, grad_output, alpha=0.01):\n",
        "    if not input.is_cuda or not grad_output.is_cuda:\n",
        "        raise ValueError(\"Tensors must be on CUDA\")\n",
        "\n",
        "    if not input.is_contiguous():\n",
        "        input = input.contiguous()\n",
        "    if not grad_output.is_contiguous():\n",
        "        grad_output = grad_output.contiguous()\n",
        "\n",
        "    grad_input = torch.empty_like(grad_output)\n",
        "\n",
        "    cuda_lib.launch_leaky_relu_backward(\n",
        "        ctypes.c_void_p(input.data_ptr()),\n",
        "        ctypes.c_void_p(grad_output.data_ptr()),\n",
        "        ctypes.c_void_p(grad_input.data_ptr()),\n",
        "        ctypes.c_int(input.numel()),\n",
        "        ctypes.c_float(alpha)\n",
        "    )\n",
        "\n",
        "    return grad_input\n",
        "\n",
        "cuda_lib.launch_sigmoid.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int\n",
        "]\n",
        "\n",
        "def sigmoid_forward(input):\n",
        "    if not input.is_cuda:\n",
        "        raise ValueError(\"Tensor must be on CUDA\")\n",
        "    if not input.is_contiguous():\n",
        "        input = input.contiguous()\n",
        "\n",
        "    output = torch.empty_like(input)\n",
        "\n",
        "    cuda_lib.launch_sigmoid(\n",
        "        ctypes.c_void_p(input.data_ptr()),\n",
        "        ctypes.c_void_p(output.data_ptr()),\n",
        "        ctypes.c_int(input.numel())\n",
        "    )\n",
        "\n",
        "    return output\n",
        "\n",
        "cuda_lib.launch_sigmoid_backward.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int\n",
        "]\n",
        "\n",
        "def sigmoid_backward(output, grad_output):\n",
        "    if not output.is_cuda or not grad_output.is_cuda:\n",
        "        raise ValueError(\"Tensors must be on CUDA\")\n",
        "\n",
        "    if not output.is_contiguous():\n",
        "        output = output.contiguous()\n",
        "    if not grad_output.is_contiguous():\n",
        "        grad_output = grad_output.contiguous()\n",
        "\n",
        "    grad_input = torch.empty_like(grad_output)\n",
        "\n",
        "    cuda_lib.launch_sigmoid_backward(\n",
        "        ctypes.c_void_p(output.data_ptr()),\n",
        "        ctypes.c_void_p(grad_output.data_ptr()),\n",
        "        ctypes.c_void_p(grad_input.data_ptr()),\n",
        "        ctypes.c_int(output.numel())\n",
        "    )\n",
        "\n",
        "    return grad_input\n",
        "\n",
        "cuda_lib.launch_tanh.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int\n",
        "]\n",
        "\n",
        "def tanh_forward(input):\n",
        "    if not input.is_cuda:\n",
        "        raise ValueError(\"Tensor must be on CUDA\")\n",
        "    if not input.is_contiguous():\n",
        "        input = input.contiguous()\n",
        "\n",
        "    output = torch.empty_like(input)\n",
        "\n",
        "    cuda_lib.launch_tanh(\n",
        "        ctypes.c_void_p(input.data_ptr()),\n",
        "        ctypes.c_void_p(output.data_ptr()),\n",
        "        ctypes.c_int(input.numel())\n",
        "    )\n",
        "\n",
        "    return output\n",
        "\n",
        "cuda_lib.launch_tanh_backward.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int\n",
        "]\n",
        "\n",
        "def tanh_backward(output, grad_output):\n",
        "    if not output.is_cuda or not grad_output.is_cuda:\n",
        "        raise ValueError(\"Tensors must be on CUDA\")\n",
        "\n",
        "    if not output.is_contiguous():\n",
        "        output = output.contiguous()\n",
        "    if not grad_output.is_contiguous():\n",
        "        grad_output = grad_output.contiguous()\n",
        "\n",
        "    grad_input = torch.empty_like(grad_output)\n",
        "\n",
        "    cuda_lib.launch_tanh_backward(\n",
        "        ctypes.c_void_p(output.data_ptr()),\n",
        "        ctypes.c_void_p(grad_output.data_ptr()),\n",
        "        ctypes.c_void_p(grad_input.data_ptr()),\n",
        "        ctypes.c_int(output.numel())\n",
        "    )\n",
        "\n",
        "    return grad_input\n",
        "\n",
        "# OPTIMIZED CODE GOES HERE\n",
        "\n",
        "cuda_lib.launch_conv_transpose2d_optimized.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int\n",
        "]\n",
        "\n",
        "def conv_transpose2d_optimized_forward(input, weight, stride, padding):\n",
        "\n",
        "    input = input.contiguous()\n",
        "    weight = weight.contiguous()\n",
        "    batch, in_channels, H_in, W_in = input.shape\n",
        "    _, out_channels, kH, kW = weight.shape\n",
        "    H_out = (H_in - 1) * stride - 2 * padding + kH\n",
        "    W_out = (W_in - 1) * stride - 2 * padding + kW\n",
        "    output_shape = (batch, out_channels, H_out, W_out)\n",
        "    output = torch.empty(output_shape, device=input.device, dtype=input.dtype)\n",
        "\n",
        "    cuda_lib.launch_conv_transpose2d_optimized(\n",
        "        ctypes.c_void_p(input.data_ptr()),\n",
        "        ctypes.c_void_p(weight.data_ptr()),\n",
        "        ctypes.c_void_p(output.data_ptr()),\n",
        "        ctypes.c_int(batch),\n",
        "        ctypes.c_int(in_channels),\n",
        "        ctypes.c_int(out_channels),\n",
        "        ctypes.c_int(H_in),\n",
        "        ctypes.c_int(W_in),\n",
        "        ctypes.c_int(H_out),\n",
        "        ctypes.c_int(W_out),\n",
        "        ctypes.c_int(kH),\n",
        "        ctypes.c_int(kW),\n",
        "        ctypes.c_int(stride),\n",
        "        ctypes.c_int(padding)\n",
        "    )\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "cuda_lib.launch_conv_transpose2d_backward_optimized.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int\n",
        "]\n",
        "\n",
        "def conv_transpose2d_optimized_backward(d_output, input, weight, stride, padding):\n",
        "    input = input.contiguous()\n",
        "    weight = weight.contiguous()\n",
        "    d_output = d_output.contiguous()\n",
        "\n",
        "    batch, in_channels, H_in, W_in = input.shape\n",
        "    _, out_channels, kH, kW = weight.shape\n",
        "    H_out = (H_in - 1) * stride - 2 * padding + kH\n",
        "    W_out = (W_in - 1) * stride - 2 * padding + kW\n",
        "\n",
        "    d_input = torch.zeros_like(input)\n",
        "    d_weight = torch.zeros_like(weight)\n",
        "\n",
        "    cuda_lib.launch_conv_transpose2d_backward_optimized(\n",
        "        ctypes.c_void_p(d_output.data_ptr()),\n",
        "        ctypes.c_void_p(input.data_ptr()),\n",
        "        ctypes.c_void_p(weight.data_ptr()),\n",
        "        ctypes.c_void_p(d_input.data_ptr()),\n",
        "        ctypes.c_void_p(d_weight.data_ptr()),\n",
        "        ctypes.c_int(batch), ctypes.c_int(in_channels), ctypes.c_int(out_channels),\n",
        "        ctypes.c_int(H_in), ctypes.c_int(W_in),\n",
        "        ctypes.c_int(H_out), ctypes.c_int(W_out),\n",
        "        ctypes.c_int(kH), ctypes.c_int(kW),\n",
        "        ctypes.c_int(stride), ctypes.c_int(padding)\n",
        "    )\n",
        "\n",
        "    return d_input, d_weight\n",
        "\n",
        "\n",
        "# Load the optimized CUDA library\n",
        "cuda_lib.launch_conv2d_forward_optimized.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int\n",
        "]\n",
        "\n",
        "def conv2d_optimized_forward(input, weight, stride, padding):\n",
        "    input = input.contiguous()\n",
        "    weight = weight.contiguous()\n",
        "    batch, in_channels, H_in, W_in = input.shape\n",
        "    out_channels, _, kH, kW = weight.shape\n",
        "    H_out = (H_in + 2 * padding - kH) // stride + 1\n",
        "    W_out = (W_in + 2 * padding - kW) // stride + 1\n",
        "    output = torch.empty((batch, out_channels, H_out, W_out), device=input.device, dtype=input.dtype)\n",
        "\n",
        "    cuda_lib.launch_conv2d_forward_optimized(\n",
        "        ctypes.c_void_p(input.data_ptr()),\n",
        "        ctypes.c_void_p(weight.data_ptr()),\n",
        "        ctypes.c_void_p(output.data_ptr()),\n",
        "        ctypes.c_int(batch), ctypes.c_int(in_channels), ctypes.c_int(out_channels),\n",
        "        ctypes.c_int(H_in), ctypes.c_int(W_in),\n",
        "        ctypes.c_int(H_out), ctypes.c_int(W_out),\n",
        "        ctypes.c_int(kH), ctypes.c_int(kW),\n",
        "        ctypes.c_int(stride), ctypes.c_int(padding)\n",
        "    )\n",
        "\n",
        "    return output\n",
        "\n",
        "# Wrapper for backward pass\n",
        "cuda_lib.launch_conv2d_backward_optimized.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int,\n",
        "    ctypes.c_int, ctypes.c_int\n",
        "]\n",
        "\n",
        "def conv2d_optimized_backward(d_output, input, weight, stride, padding):\n",
        "    input = input.contiguous()\n",
        "    weight = weight.contiguous()\n",
        "    d_output = d_output.contiguous()\n",
        "\n",
        "    batch, in_channels, H_in, W_in = input.shape\n",
        "    out_channels, _, kH, kW = weight.shape\n",
        "    H_out = (H_in + 2 * padding - kH) // stride + 1\n",
        "    W_out = (W_in + 2 * padding - kW) // stride + 1\n",
        "\n",
        "    d_input = torch.zeros_like(input)\n",
        "    d_weight = torch.zeros_like(weight)\n",
        "\n",
        "    cuda_lib.launch_conv2d_backward_optimized(\n",
        "        ctypes.c_void_p(d_output.data_ptr()),\n",
        "        ctypes.c_void_p(input.data_ptr()),\n",
        "        ctypes.c_void_p(weight.data_ptr()),\n",
        "        ctypes.c_void_p(d_input.data_ptr()),\n",
        "        ctypes.c_void_p(d_weight.data_ptr()),\n",
        "        ctypes.c_int(batch), ctypes.c_int(in_channels), ctypes.c_int(out_channels),\n",
        "        ctypes.c_int(H_in), ctypes.c_int(W_in),\n",
        "        ctypes.c_int(H_out), ctypes.c_int(W_out),\n",
        "        ctypes.c_int(kH), ctypes.c_int(kW),\n",
        "        ctypes.c_int(stride), ctypes.c_int(padding)\n",
        "    )\n",
        "\n",
        "    return d_input, d_weight\n",
        "\n",
        "cuda_lib.launch_batchnorm_forward_optimized.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_float\n",
        "]\n",
        "\n",
        "def batchnorm_optimized_forward(input, gamma, beta, eps):\n",
        "\n",
        "    input = input.contiguous()\n",
        "    gamma = gamma.contiguous()\n",
        "    beta = beta.contiguous()\n",
        "\n",
        "    N, C, H, W = input.shape\n",
        "    output = torch.empty_like(input)\n",
        "\n",
        "    cuda_lib.launch_batchnorm_forward_optimized(\n",
        "        ctypes.c_void_p(input.data_ptr()),\n",
        "        ctypes.c_void_p(output.data_ptr()),\n",
        "        ctypes.c_void_p(gamma.data_ptr()),\n",
        "        ctypes.c_void_p(beta.data_ptr()),\n",
        "        ctypes.c_int(N), ctypes.c_int(C), ctypes.c_int(H), ctypes.c_int(W),\n",
        "        ctypes.c_float(eps)\n",
        "    )\n",
        "\n",
        "    return output\n",
        "\n",
        "cuda_lib.launch_batchnorm_backward_optimized.argtypes = [\n",
        "    ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_void_p, ctypes.c_void_p,\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_float\n",
        "]\n",
        "\n",
        "def batchnorm_optimized_backward(grad_output, input, gamma, beta, eps):\n",
        "    grad_output = grad_output.contiguous()\n",
        "    input = input.contiguous()\n",
        "    gamma = gamma.contiguous()\n",
        "    beta = beta.contiguous()\n",
        "\n",
        "    N, C, H, W = input.shape\n",
        "    grad_input = torch.empty_like(grad_output)\n",
        "\n",
        "    cuda_lib.launch_batchnorm_backward_optimized(\n",
        "        ctypes.c_void_p(grad_output.data_ptr()),\n",
        "        ctypes.c_void_p(input.data_ptr()),\n",
        "        ctypes.c_void_p(gamma.data_ptr()),\n",
        "        ctypes.c_void_p(beta.data_ptr()),\n",
        "        ctypes.c_void_p(grad_input.data_ptr()),\n",
        "        ctypes.c_int(N), ctypes.c_int(C), ctypes.c_int(H), ctypes.c_int(W),\n",
        "        ctypes.c_float(eps)\n",
        "    )\n",
        "\n",
        "    return grad_input\n",
        "\n"
      ],
      "metadata": {
        "id": "xp1gxmKy6ixM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import EMNIST\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Custom autograd Functions - calling CUDA kernels.\n",
        "\n",
        "class DoubleFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        output = input.clone()\n",
        "        double_elements(output)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        grad_input = double_elements_backward(grad_output)\n",
        "        return grad_input\n",
        "\n",
        "class CustomDoubleLayer(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return DoubleFunction.apply(input)\n",
        "\n",
        "class CustomConvTranspose2dFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, stride, padding):\n",
        "        ctx.stride = stride\n",
        "        ctx.padding = padding\n",
        "        ctx.save_for_backward(input, weight)\n",
        "        output = conv_transpose_general_forward(input, weight, stride, padding)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, weight = ctx.saved_tensors\n",
        "        d_input, d_weight = conv_transpose2d_backward(grad_output, input, weight, ctx.stride, ctx.padding)\n",
        "        return d_input, d_weight, None, None\n",
        "\n",
        "class CustomConvTranspose2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
        "        self.weight = nn.Parameter(torch.randn(in_channels, out_channels, *self.kernel_size))\n",
        "        nn.init.normal_(self.weight, 0.0, 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return CustomConvTranspose2dFunction.apply(x, self.weight, self.stride, self.padding)\n",
        "\n",
        "class CustomBatchNorm2dFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, gamma, beta, eps):\n",
        "        ctx.eps = eps\n",
        "        N, C, H, W = input.shape\n",
        "        ctx.save_for_backward(input, gamma, beta)\n",
        "\n",
        "        mean = input.mean(dim=(0, 2, 3), keepdim=True)\n",
        "        var = input.var(dim=(0, 2, 3), keepdim=True, unbiased=False)\n",
        "\n",
        "        output = batchnorm_forward(input, gamma, beta, eps, N, C, H, W)\n",
        "\n",
        "        ctx.save_for_backward(input, mean, var, gamma)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, mean, var, gamma = ctx.saved_tensors\n",
        "        d_input, d_gamma, d_beta = batchnorm2d_backward(grad_output, input, mean, var, gamma, ctx.eps)\n",
        "        return d_input, d_gamma, d_beta, None\n",
        "\n",
        "\n",
        "class CustomBatchNorm2d(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5):\n",
        "        super(CustomBatchNorm2d, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
        "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return CustomBatchNorm2dFunction.apply(input, self.gamma, self.beta, self.eps)\n",
        "\n",
        "class CustomConv2dFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, stride, padding):\n",
        "        ctx.stride = stride\n",
        "        ctx.padding = padding\n",
        "        ctx.save_for_backward(input, weight)\n",
        "        output = conv2d_forward(input, weight, stride, padding)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, weight = ctx.saved_tensors\n",
        "        d_input, d_weight = conv2d_backward(grad_output, input, weight, ctx.stride, ctx.padding)\n",
        "        return d_input, d_weight, None, None\n",
        "\n",
        "\n",
        "class CustomConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n",
        "        super(CustomConv2d, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
        "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, *self.kernel_size))\n",
        "        nn.init.normal_(self.weight, 0.0, 0.02)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_channels))\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = CustomConv2dFunction.apply(x, self.weight, self.stride, self.padding)\n",
        "        return output\n",
        "\n",
        "class CustomReLUFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        output = relu_forward(input)\n",
        "        ctx.save_for_backward(input)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = relu_backward(input, grad_output)\n",
        "        return grad_input\n",
        "\n",
        "class CustomReLUModule(torch.nn.Module):\n",
        "    def forward(self, input):\n",
        "        return CustomReLUFunction.apply(input)\n",
        "\n",
        "class CustomLeakyReLUFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, alpha=0.01):\n",
        "        output = leaky_relu_forward(input, alpha)\n",
        "        ctx.save_for_backward(input)\n",
        "        ctx.alpha = alpha\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = leaky_relu_backward(input, grad_output, ctx.alpha)\n",
        "        return grad_input, None\n",
        "\n",
        "class CustomLeakyReLUModule(torch.nn.Module):\n",
        "    def __init__(self, alpha=0.01):\n",
        "        super(CustomLeakyReLUModule, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, input):\n",
        "        return CustomLeakyReLUFunction.apply(input, self.alpha)\n",
        "\n",
        "# Future Work\n",
        "class CustomSigmoidFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        output = sigmoid_forward(input)\n",
        "        ctx.save_for_backward(output)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output, = ctx.saved_tensors\n",
        "        grad_input = sigmoid_backward(output, grad_output)\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "class CustomSigmoidModule(torch.nn.Module):\n",
        "    def forward(self, input):\n",
        "        return CustomSigmoidFunction.apply(input)\n",
        "\n",
        "# Future Work\n",
        "class CustomTanhFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        output = tanh_forward(input)\n",
        "        ctx.save_for_backward(output)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output, = ctx.saved_tensors\n",
        "        grad_input = tanh_backward(output, grad_output)\n",
        "        return grad_input\n",
        "\n",
        "class CustomTanhModule(torch.nn.Module):\n",
        "    def forward(self, input):\n",
        "        return CustomTanhFunction.apply(input)\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5\n",
        "    np_img = img.cpu().numpy()\n",
        "    plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# OPTIMIZED CODE GOES HERE\n",
        "\n",
        "class CustomConvTranspose2dFunction_OPTIMIZED(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, stride, padding):\n",
        "        ctx.stride = stride\n",
        "        ctx.padding = padding\n",
        "        ctx.save_for_backward(input, weight)\n",
        "        output = conv_transpose2d_optimized_forward(input, weight, stride, padding)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, weight = ctx.saved_tensors\n",
        "        d_input, d_weight = conv_transpose2d_optimized_backward(grad_output, input, weight, ctx.stride, ctx.padding)\n",
        "        return d_input, d_weight, None, None\n",
        "\n",
        "class CustomConvTranspose2d_OPTIMIZED(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
        "        self.weight = nn.Parameter(torch.randn(in_channels, out_channels, *self.kernel_size))\n",
        "        nn.init.normal_(self.weight, 0.0, 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return CustomConvTranspose2dFunction_OPTIMIZED.apply(x, self.weight, self.stride, self.padding)\n",
        "\n",
        "class CustomConv2dFunction_OPTIMIZED(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, stride, padding):\n",
        "        ctx.stride = stride\n",
        "        ctx.padding = padding\n",
        "        ctx.save_for_backward(input, weight)\n",
        "        output = conv2d_optimized_forward(input, weight, stride, padding)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, weight = ctx.saved_tensors\n",
        "        d_input, d_weight = conv2d_optimized_backward(grad_output, input, weight, ctx.stride, ctx.padding)\n",
        "        return d_input, d_weight, None, None\n",
        "\n",
        "class CustomConv2d_OPTIMIZED(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n",
        "        super(CustomConv2d_OPTIMIZED, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
        "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, *self.kernel_size))\n",
        "        nn.init.normal_(self.weight, 0.0, 0.02)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_channels))\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        return CustomConv2dFunction_OPTIMIZED.apply(x, self.weight, self.stride, self.padding)\n",
        "\n",
        "\n",
        "class CustomBatchNorm2dFunction_OPTIMIZED(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, gamma, beta, eps):\n",
        "        ctx.eps = eps\n",
        "        ctx.save_for_backward(input, gamma, beta)\n",
        "        output = batchnorm_optimized_forward(input, gamma, beta, eps)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, gamma, beta = ctx.saved_tensors\n",
        "        grad_input = batchnorm_optimized_backward(grad_output, input, gamma, beta, ctx.eps)\n",
        "        return grad_input, None, None, None\n",
        "\n",
        "\n",
        "class CustomBatchNorm2d_OPTIMIZED(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5):\n",
        "        super(CustomBatchNorm2d_OPTIMIZED, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
        "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return CustomBatchNorm2dFunction_OPTIMIZED.apply(input, self.gamma, self.beta, self.eps)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGkW178X6q8W",
        "outputId": "d7d49e54-d616-4f47-f516-204a54a2be09"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz, ngf, nc):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer1 = CustomConvTranspose2d_OPTIMIZED(nz, ngf * 8, kernel_size=4, stride=1, padding=0)\n",
        "        self.layer2 = CustomConvTranspose2d_OPTIMIZED(ngf * 8, ngf * 4, kernel_size=4, stride=2, padding=1)\n",
        "        self.layer3 = CustomConvTranspose2d_OPTIMIZED(ngf * 4, ngf * 2, kernel_size=4, stride=2, padding=1)\n",
        "        self.layer4 = CustomConvTranspose2d_OPTIMIZED(ngf * 2, ngf, kernel_size=4, stride=2, padding=1)\n",
        "        self.layer5 = CustomConvTranspose2d_OPTIMIZED(ngf, nc, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        self.relu = CustomReLUModule()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.bn2 = CustomBatchNorm2d_OPTIMIZED(ngf * 4)\n",
        "        self.bn3 = CustomBatchNorm2d_OPTIMIZED(ngf * 2)\n",
        "        self.bn4 = CustomBatchNorm2d_OPTIMIZED(ngf)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer5(x)\n",
        "        x = self.tanh(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, nc, ndf):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            CustomConv2d_OPTIMIZED(nc, ndf, 4, 2, 1, bias=False),\n",
        "            CustomLeakyReLUModule(0.2),\n",
        "            CustomConv2d_OPTIMIZED(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            CustomBatchNorm2d_OPTIMIZED(ndf * 2),\n",
        "            CustomLeakyReLUModule(0.2),\n",
        "            CustomConv2d_OPTIMIZED(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            CustomBatchNorm2d_OPTIMIZED(ndf * 4),\n",
        "            CustomLeakyReLUModule(0.2),\n",
        "            CustomConv2d_OPTIMIZED(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            CustomBatchNorm2d_OPTIMIZED(ndf * 8),\n",
        "            CustomLeakyReLUModule(0.2),\n",
        "            CustomConv2d_OPTIMIZED(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x).view(-1)"
      ],
      "metadata": {
        "id": "F-jjfUrU2O-0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "\n",
        "# Hyper-params\n",
        "nz = 100\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "nc = 1  # GRAYSCALE\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(64),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "\n",
        "dataset = EMNIST(root='./data', split='balanced', download=True, transform=transform)\n",
        "indices = torch.arange(10000)\n",
        "dataset = Subset(dataset, indices)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# GPU Set up\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "netG = Generator(nz, ngf, nc).to(device)\n",
        "netD = Discriminator(nc, ndf).to(device)\n",
        "\n",
        "# Loss Function & Optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "print(\"Starting Training on GPU with Custom CUDA Kernels...\\n\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (real_images, _) in enumerate(dataloader):\n",
        "        real_images = real_images.to(device)\n",
        "        b_size = real_images.size(0)\n",
        "\n",
        "        # Train Discriminator\n",
        "        netD.zero_grad()\n",
        "        real_labels = torch.full((b_size,), 1, dtype=torch.float, device=device)\n",
        "        output_real = netD(real_images)\n",
        "        lossD_real = criterion(output_real, real_labels)\n",
        "        lossD_real.backward()\n",
        "\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        fake_images = netG(noise)\n",
        "        fake_labels = torch.full((b_size,), 0, dtype=torch.float, device=device)\n",
        "        output_fake = netD(fake_images.detach())\n",
        "        lossD_fake = criterion(output_fake, fake_labels)\n",
        "        lossD_fake.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        # Train Generator\n",
        "        netG.zero_grad()\n",
        "        gen_labels = torch.full((b_size,), 1, dtype=torch.float, device=device)\n",
        "        output_gen = netD(fake_images)\n",
        "        lossG = criterion(output_gen, gen_labels)\n",
        "        lossG.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print(f\"[{epoch+1}/{epochs}] [{i}/{len(dataloader)}] Loss_D: {(lossD_real+lossD_fake).item():.4f} | Loss_G: {lossG.item():.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training completed in {end_time - start_time:.2f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-ckfch-9cqb",
        "outputId": "d46ae2a5-7270-4081-a563-24e17dcf4569"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Starting Training on GPU with Custom CUDA Kernels...\n",
            "\n",
            "[1/10] [0/157] Loss_D: 1.3708 | Loss_G: 0.6931\n",
            "[1/10] [50/157] Loss_D: 1.8010 | Loss_G: 0.6698\n",
            "[1/10] [100/157] Loss_D: 1.3970 | Loss_G: 0.3133\n",
            "[1/10] [150/157] Loss_D: 1.0064 | Loss_G: 0.3133\n",
            "[2/10] [0/157] Loss_D: 1.4644 | Loss_G: 0.5982\n",
            "[2/10] [50/157] Loss_D: 0.9958 | Loss_G: 0.6778\n",
            "[2/10] [100/157] Loss_D: 1.3970 | Loss_G: 0.3133\n",
            "[2/10] [150/157] Loss_D: 1.0064 | Loss_G: 0.3133\n",
            "[3/10] [0/157] Loss_D: 1.0746 | Loss_G: 0.6931\n",
            "[3/10] [50/157] Loss_D: 1.0064 | Loss_G: 0.3133\n",
            "[3/10] [100/157] Loss_D: 1.0064 | Loss_G: 0.3133\n",
            "[3/10] [150/157] Loss_D: 1.0064 | Loss_G: 0.3133\n",
            "[4/10] [0/157] Loss_D: 1.1687 | Loss_G: 0.3824\n",
            "[4/10] [50/157] Loss_D: 0.9958 | Loss_G: 0.6931\n",
            "[4/10] [100/157] Loss_D: 1.0064 | Loss_G: 0.3133\n",
            "[4/10] [150/157] Loss_D: 1.3708 | Loss_G: 0.6779\n",
            "[5/10] [0/157] Loss_D: 1.3708 | Loss_G: 0.4899\n",
            "[5/10] [50/157] Loss_D: 1.0064 | Loss_G: 0.3133\n",
            "[5/10] [100/157] Loss_D: 1.0870 | Loss_G: 0.6778\n",
            "[5/10] [150/157] Loss_D: 1.0064 | Loss_G: 0.3133\n",
            "[6/10] [0/157] Loss_D: 1.4179 | Loss_G: 0.6722\n",
            "[6/10] [50/157] Loss_D: 1.0064 | Loss_G: 0.3133\n",
            "[6/10] [100/157] Loss_D: 1.0064 | Loss_G: 0.3133\n",
            "[6/10] [150/157] Loss_D: 0.9958 | Loss_G: 0.6778\n",
            "[7/10] [0/157] Loss_D: 1.2284 | Loss_G: 0.6778\n",
            "[7/10] [50/157] Loss_D: 1.4018 | Loss_G: 0.6776\n",
            "[7/10] [100/157] Loss_D: 1.4018 | Loss_G: 0.3133\n",
            "[7/10] [150/157] Loss_D: 1.0064 | Loss_G: 0.6776\n",
            "[8/10] [0/157] Loss_D: 1.0746 | Loss_G: 0.6778\n",
            "[8/10] [50/157] Loss_D: 1.3708 | Loss_G: 0.6779\n",
            "[8/10] [100/157] Loss_D: 1.4051 | Loss_G: 0.3133\n",
            "[8/10] [150/157] Loss_D: 0.9958 | Loss_G: 0.3133\n",
            "[9/10] [0/157] Loss_D: 1.3708 | Loss_G: 0.6779\n",
            "[9/10] [50/157] Loss_D: 1.4018 | Loss_G: 0.3133\n",
            "[9/10] [100/157] Loss_D: 1.0064 | Loss_G: 0.3133\n",
            "[9/10] [150/157] Loss_D: 1.0064 | Loss_G: 0.6776\n",
            "[10/10] [0/157] Loss_D: 1.0700 | Loss_G: 0.3824\n",
            "[10/10] [50/157] Loss_D: 1.3970 | Loss_G: 0.3133\n",
            "[10/10] [100/157] Loss_D: 1.0064 | Loss_G: 0.3133\n",
            "[10/10] [150/157] Loss_D: 1.0064 | Loss_G: 0.3133\n",
            "Training completed in 47.67 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "fake_images = netG(noise).detach()\n",
        "generated_grid = torchvision.utils.make_grid(fake_images[:64], nrow=8, padding=2)\n",
        "imshow(generated_grid)"
      ],
      "metadata": {
        "id": "iD-WPmhO-vue",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "78b50e62-9914-4d4b-d23f-a3c5e652c594"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size = 16\n",
        "in_channels = 64\n",
        "out_channels = 128\n",
        "H_in = 32\n",
        "W_in = 32\n",
        "kernel_size = 4\n",
        "stride = 2\n",
        "padding = 1\n",
        "\n",
        "input_tensor = torch.randn(batch_size, in_channels, H_in, W_in, device=device, requires_grad=True)\n",
        "weight_tensor = torch.randn(out_channels, in_channels, kernel_size, kernel_size, device=device, requires_grad=True)\n",
        "\n",
        "conv_old = CustomConv2d(in_channels, out_channels, kernel_size, stride, padding).to(device)\n",
        "conv_opt = CustomConv2d_OPTIMIZED(in_channels, out_channels, kernel_size, stride, padding).to(device)\n",
        "\n",
        "convT_old = CustomConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding).to(device)\n",
        "convT_opt = CustomConvTranspose2d_OPTIMIZED(in_channels, out_channels, kernel_size, stride, padding).to(device)\n",
        "\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "def benchmark_model(model, input_tensor, weight_tensor, name):\n",
        "    start_time = time.time()\n",
        "    output = model(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "    forward_time = time.time() - start_time\n",
        "\n",
        "    grad_output = torch.randn_like(output, device=device)\n",
        "    start_time = time.time()\n",
        "    output.backward(grad_output, retain_graph=True)\n",
        "    torch.cuda.synchronize()\n",
        "    backward_time = time.time() - start_time\n",
        "\n",
        "    print(f\"{name} - Forward: {forward_time:.6f} sec | Backward: {backward_time:.6f} sec\")\n",
        "    return forward_time, backward_time\n",
        "\n",
        "print(\"Benchmarking Conv2d (Old vs. Optimized)...\")\n",
        "fwd_old, bwd_old = benchmark_model(conv_old, input_tensor, weight_tensor, \"Conv2d Old CUDA\")\n",
        "fwd_opt, bwd_opt = benchmark_model(conv_opt, input_tensor, weight_tensor, \"Conv2d Optimized CUDA\")\n",
        "\n",
        "print(\"\\nBenchmarking ConvTranspose2d (Old vs. Optimized)...\")\n",
        "fwdT_old, bwdT_old = benchmark_model(convT_old, input_tensor, weight_tensor, \"ConvTranspose2d Old CUDA\")\n",
        "fwdT_opt, bwdT_opt = benchmark_model(convT_opt, input_tensor, weight_tensor, \"ConvTranspose2d Optimized CUDA\")\n",
        "speedup_conv_fwd = fwd_old / fwd_opt\n",
        "speedup_conv_bwd = bwd_old / bwd_opt\n",
        "speedup_convT_fwd = fwdT_old / fwdT_opt\n",
        "speedup_convT_bwd = bwdT_old / bwdT_opt\n",
        "\n",
        "print(\"\\n Speedup Summary:\")\n",
        "print(f\"Conv2d Speedup - Forward: {speedup_conv_fwd:.2f}x | Backward: {speedup_conv_bwd:.2f}x\")\n",
        "print(f\"ConvTranspose2d Speedup - Forward: {speedup_convT_fwd:.2f}x | Backward: {speedup_convT_bwd:.2f}x\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvoFYZP5N2cD",
        "outputId": "205b0e18-c84c-4945-cd9e-a05a74911d8a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmarking Conv2d (Old vs. Optimized)...\n",
            "Conv2d Old CUDA - Forward: 0.000568 sec | Backward: 0.001791 sec\n",
            "Conv2d Optimized CUDA - Forward: 0.000178 sec | Backward: 0.000413 sec\n",
            "\n",
            "Benchmarking ConvTranspose2d (Old vs. Optimized)...\n",
            "ConvTranspose2d Old CUDA - Forward: 0.000188 sec | Backward: 0.000415 sec\n",
            "ConvTranspose2d Optimized CUDA - Forward: 0.000176 sec | Backward: 0.000368 sec\n",
            "\n",
            " Speedup Summary:\n",
            "Conv2d Speedup - Forward: 3.19x | Backward: 4.33x\n",
            "ConvTranspose2d Speedup - Forward: 1.07x | Backward: 1.13x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size = 64\n",
        "num_features = 128\n",
        "H = 32\n",
        "W = 32\n",
        "\n",
        "input_tensor = torch.randn(batch_size, num_features, H, W, device=device, requires_grad=True)\n",
        "gamma = torch.ones(num_features, device=device, requires_grad=True)\n",
        "beta = torch.zeros(num_features, device=device, requires_grad=True)\n",
        "eps = 1e-5\n",
        "batchnorm_old = CustomBatchNorm2d(num_features).to(device)\n",
        "batchnorm_opt = CustomBatchNorm2d_OPTIMIZED(num_features).to(device)\n",
        "\n",
        "def benchmark_model(model, input_tensor, gamma, beta, eps, name):\n",
        "    start_time = time.time()\n",
        "    output = model(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "    forward_time = time.time() - start_time\n",
        "    grad_output = torch.randn_like(output, device=device)\n",
        "    start_time = time.time()\n",
        "    output.backward(grad_output, retain_graph=True)\n",
        "    torch.cuda.synchronize()\n",
        "    backward_time = time.time() - start_time\n",
        "\n",
        "    print(f\"{name} - Forward: {forward_time:.6f} sec | Backward: {backward_time:.6f} sec\")\n",
        "    return forward_time, backward_time\n",
        "\n",
        "# Run benchmarks\n",
        "print(\"Benchmarking BatchNorm2d (Old vs. Optimized)...\")\n",
        "fwd_old, bwd_old = benchmark_model(batchnorm_old, input_tensor, gamma, beta, eps, \"BatchNorm2d Old CUDA\")\n",
        "fwd_opt, bwd_opt = benchmark_model(batchnorm_opt, input_tensor, gamma, beta, eps, \"BatchNorm2d Optimized CUDA\")\n",
        "\n",
        "# Compute Speedups\n",
        "speedup_fwd = fwd_old / fwd_opt\n",
        "speedup_bwd = bwd_old / bwd_opt\n",
        "\n",
        "print(\"\\nSpeedup Summary:\")\n",
        "print(f\"BatchNorm2d Speedup - Forward: {speedup_fwd:.2f}x | Backward: {speedup_bwd:.2f}x\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GsgN7m6SRG6",
        "outputId": "381da5a6-ba2e-4ec6-841b-93d75038175d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmarking BatchNorm2d (Old vs. Optimized)...\n",
            "BatchNorm2d Old CUDA - Forward: 0.001209 sec | Backward: 0.001335 sec\n",
            "BatchNorm2d Optimized CUDA - Forward: 0.000479 sec | Backward: 0.000710 sec\n",
            "\n",
            "Speedup Summary:\n",
            "BatchNorm2d Speedup - Forward: 2.53x | Backward: 1.88x\n"
          ]
        }
      ]
    }
  ]
}